# HyperRAG Code Flow Documentation

This document provides a high-level overview of the HyperRAG pipeline, from data preprocessing through hypergraph construction, question extraction, and response generation.

## Table of Contents
1. [Step 0: Data Preprocessing](#step-0-data-preprocessing)
2. [Step 1: Hypergraph Construction](#step-1-hypergraph-construction)
3. [Step 2: Question Extraction](#step-2-question-extraction)
4. [Step 3: Response Generation](#step-3-response-generation)

---

## Step 0: Data Preprocessing

**File**: `reproduce/Step_0.py`  
**Main Function**: `extract_unique_contexts(input_directory, output_directory)`

### Purpose

Extracts and deduplicates unique contexts from JSONL dataset files, ensuring each unique context appears only once for use in subsequent steps.

### Process

The `extract_unique_contexts()` function in `reproduce/Step_0.py`:

1. **Input**: Reads JSONL files from `datasets/{data_name}/` directory
2. **Processing**: 
   - Extracts the `context` field from each JSON object
   - Uses dictionary keys for automatic deduplication
   - Handles errors gracefully for malformed JSON
3. **Output**: Saves unique contexts to `caches/{data_name}/contexts/{data_name}_unique_contexts.json` as a JSON array

---

## Step 1: Hypergraph Construction

**File**: `reproduce/Step_1.py`  
**Main Functions**: `HyperRAG.insert()`, `extract_entities()`

### Purpose

Builds a HyperGraph knowledge base by extracting entities and relationships from text, creating a structured graph that connects entities through hyperedges (relationships).

### Process

#### 1. Initialization

The script initializes a `HyperRAG` instance (`hyperrag/hyperrag.py`), which sets up storage components in `HyperRAG.__post_init__()`:
- **KV Storage** (`JsonKVStorage` from `hyperrag/storage.py`): For full documents and text chunks
- **Hypergraph Storage** (`HypergraphStorage` from `hyperrag/storage.py`): Graph database for entities (vertices) and relationships (hyperedges)
- **Vector Databases** (`NanoVectorDBStorage` from `hyperrag/storage.py`): For semantic search on entities, relationships, and chunks
- **LLM Cache** (`JsonKVStorage`): Optional caching of LLM responses via `openai_complete_if_cache()` from `hyperrag/llm.py`

#### 2. Document Processing

The main insertion is triggered by `HyperRAG.insert()` → `HyperRAG.ainsert()` (`hyperrag/hyperrag.py`):

1. **Load Contexts**: Reads unique contexts from Step 0 output
2. **Text Chunking**: Uses `chunking_by_token_size()` (`hyperrag/operate.py`) to split documents into overlapping chunks (default: 1200 tokens, 100 token overlap). Each chunk gets a hash-based ID via `compute_mdhash_id()` (`hyperrag/utils.py`)
3. **Store Chunks**: Saves chunks to KV storage and vector database using embeddings generated by `openai_embedding()` (`hyperrag/llm.py`)

#### 3. Entity and Relationship Extraction

For each text chunk, the `extract_entities()` function (`hyperrag/operate.py`) performs:

1. **LLM-Based Extraction**:
   - Uses `PROMPTS["entity_extraction"]` (`hyperrag/prompt.py`) to extract entities (name, type, description, properties) via LLM calls
   - Extracts low-order relationships (2 entities) and high-order relationships (3+ entities)
   - Iterative gleaning using `PROMPTS["entity_continue_extraction"]` and `PROMPTS["entity_if_loop_extraction"]` to find missed entities

2. **Parsing**:
   - `_handle_single_entity_extraction()`: Parses entity tuples from LLM output
   - `_handle_single_relationship_extraction_low()`: Parses low-order hyperedges (2 entities)
   - `_handle_single_relationship_extraction_high()`: Parses high-order hyperedges (3+ entities)

3. **Merging**:
   - **Entity Merging** (`_merge_nodes_then_upsert()`): Combines duplicate entities, merges descriptions, selects most common type. If descriptions exceed token limits, uses `_handle_entity_summary()` or `_handle_entity_additional_properties()` to summarize via LLM
   - **Relationship Merging** (`_merge_edges_then_upsert()`): Combines duplicate relationships, averages weights, merges keywords. If descriptions/keywords exceed limits, uses `_handle_relation_summary()` or `_handle_relation_keywords_summary()` to summarize via LLM

4. **Storage**:
   - Entities stored as vertices using `HypergraphStorage.upsert_vertex()` (`hyperrag/storage.py`)
   - Relationships stored as hyperedges using `HypergraphStorage.upsert_hyperedge()` (`hyperrag/storage.py`)
   - Entity and relationship embeddings created via `openai_embedding()` and stored using `NanoVectorDBStorage.upsert()` (`hyperrag/storage.py`)

#### 4. Output Files

Generated in `caches/{data_name}/`:
- `hypergraph_chunk_entity_relation.hgdb`: Hypergraph structure
- `vdb_entities.json`: Entity vector database
- `vdb_relationships.json`: Relationship vector database
- `vdb_chunks.json`: Chunk vector database
- `kv_store_*.json`: Key-value storage for documents and chunks
- `HyperRAG.log`: Processing logs

---

## Step 2: Question Extraction

**File**: `reproduce/Step_2_extract_question.py`

### Purpose

Generates evaluation questions from unique contexts using LLM. Creates questions of varying complexity (1-stage, 2-stage, or 3-stage) to test the knowledge stored in the HyperGraph.

### Process

#### 1. Configuration
- Loads unique contexts from Step 0
- Sets parameters: number of questions, token limits, question stage (1, 2, or 3)

#### 2. Context Combination

1. **Pre-filtering**: Finds all valid context combinations that fit within token limits
   - Tries combining 3, 2, or 1 consecutive contexts
   - Checks token count for each combination using `tiktoken` library for encoding
   - Uses `truncate_context_to_tokens()` (`reproduce/Step_2_extract_question.py`) if needed to truncate while preserving sentence boundaries

2. **Random Selection**: Randomly samples valid combinations without replacement using `numpy.random.choice()`

#### 3. Question Generation

For each selected context:

1. **Prompt Formatting**: Uses different prompts from the `question_prompt` dictionary (`reproduce/Step_2_extract_question.py`) based on question stage:
   - **1-stage**: Single, focused question
   - **2-stage**: Two interconnected sub-questions (e.g., "What is X and specifically how does Y work?")
   - **3-stage**: Three interconnected sub-questions

2. **LLM Call**: Uses `llm_model_func()` which creates an OpenAI client via `get_openai_client()` (singleton pattern) and calls the OpenAI API to generate question based on context

3. **Question Extraction**: Extracts question from JSON-formatted LLM response using regex pattern matching

4. **Error Handling**: Skips invalid responses and continues to next combination

#### 4. Output Files

Generated in `caches/{data_name}/questions/`:
- `{stage}_stage.json`: JSON array of questions
- `{stage}_stage_ref.json`: JSON array of reference contexts (aligned with questions)

---

## Step 3: Response Generation

**File**: `reproduce/Step_3_response_question.py`  
**Main Functions**: `HyperRAG.aquery()`, `hyper_query()`, `naive_query()`

### Purpose

Generates answers to extracted questions using HyperRAG. Supports multiple query modes that use different retrieval strategies.

### Process

#### 1. Initialization

1. **Load Questions**: `extract_queries()` (`reproduce/Step_3_response_question.py`) reads questions from Step 2 output
2. **Initialize HyperRAG**: `HyperRAG.__init__()` (`hyperrag/hyperrag.py`) loads existing hypergraph from Step 1 and initializes vector databases
3. **Configure Mode**: Creates `QueryParam` instance (`hyperrag/base.py`) to select query mode (`"naive"`, `"hyper"`, `"hyper-lite"`, `"graph"`, or `"llm"`)

#### 2. Query Processing

The `run_queries_and_save_to_json()` function (`reproduce/Step_3_response_question.py`) processes each question using `process_query()`, which:
- Calls `rag_instance.aquery()` (`hyperrag/hyperrag.py`) - the main async query method that routes to appropriate query function based on mode
- Uses `always_get_an_event_loop()` (`hyperrag/utils.py`) to get or create async event loop
- Incrementally saves results to JSON file
- Logs errors to separate error file

#### 3. Query Modes

##### Mode: "naive" (Baseline RAG)

The `naive_query()` function (`hyperrag/operate.py`):
1. Performs vector search on chunks using `NanoVectorDBStorage.query()` (`hyperrag/storage.py`)
2. Retrieves top-k similar chunks from `text_chunks_db`
3. Truncates to token limit
4. Generates answer using `PROMPTS["naive_rag_response"]` (`hyperrag/prompt.py`)

**Characteristics**: Fast, simple, baseline performance

##### Mode: "hyper" (Full HyperRAG)

The `hyper_query()` function (`hyperrag/operate.py`) uses the full graph structure:

1. **Keyword Extraction**: Uses `PROMPTS["keywords_extraction"]` (`hyperrag/prompt.py`) with LLM to extract entity keywords and relationship keywords from query

2. **Entity-based Context Building**: `_build_entity_query_context()` (`hyperrag/operate.py`) builds context by:
   - Performing vector search on entities using `entities_vdb.query()` (`NanoVectorDBStorage`)
   - Retrieving entity details from hypergraph using `HypergraphStorage.get_vertex()` (`hyperrag/storage.py`)
   - Finding related text chunks via `_find_most_related_text_unit_from_entities()`, which uses `HypergraphStorage.get_nbr_e_of_vertex()` to get connected hyperedges
   - Finding related relationships via `_find_most_related_edges_from_entities()`, which uses `HypergraphStorage.vertex_degree()` to get entity connectivity
   - Formatting as structured context (entities, relationships, sources tables)

3. **Relationship-based Context Building**: `_build_relation_query_context()` (`hyperrag/operate.py`) builds context by:
   - Performing vector search on relationships using `relationships_vdb.query()` (`NanoVectorDBStorage`)
   - Retrieving relationship details using `HypergraphStorage.get_hyperedge()` (`hyperrag/storage.py`)
   - Finding related entities via `_find_most_related_entities_from_relationships()` using `HypergraphStorage.vertex_degree()`
   - Finding related text chunks via `_find_related_text_unit_from_relationships()`
   - Formatting as structured context

4. **Context Combination**: `combine_contexts()` and `deduplicate_by_key()` (`hyperrag/operate.py`) combine entity and relationship contexts, deduplicating entities, relationships, and chunks

5. **Generate Answer**: Uses `PROMPTS["rag_response"]` and `PROMPTS["rag_define"]` (`hyperrag/prompt.py`) with rich structured context to generate comprehensive answer

**Characteristics**: Most comprehensive, highest accuracy, uses full graph structure

##### Mode: "hyper-lite"

The `hyper_query_lite()` function (`hyperrag/operate.py`):
- Similar to "hyper" mode but only uses entity-based retrieval via `_build_entity_query_context()`
- Faster than full hyper mode, less comprehensive context

##### Mode: "graph"

The `graph_query()` function (`hyperrag/operate.py`):
- Focuses on pairwise relationships only
- Filters to binary relationships (2 entities)
- Returns relationship pairs directly

##### Mode: "llm"

The `llm_query()` function (`hyperrag/operate.py`):
- Direct LLM call with no retrieval
- No context building
- Pure LLM baseline

#### 4. Output Files

Generated in `caches/{data_name}/response/`:
- `{mode}_{stage}_stage_result.json`: JSON array of query-result pairs
- `{mode}_{stage}_stage_errors.json`: JSON array of error records (if any)

---

## Complete Pipeline Flow

```
Dataset (JSONL)
    ↓
Step 0: Data Preprocessing
    → extract_unique_contexts()
    → Output: caches/{data_name}/contexts/{data_name}_unique_contexts.json
    ↓
Step 1: Hypergraph Construction
    → HyperRAG.insert() → ainsert()
    → chunking_by_token_size()
    → extract_entities()
    → _merge_nodes_then_upsert() (entities)
    → _merge_edges_then_upsert() (relationships)
    → Output: hypergraph.hgdb, vdb_*.json, kv_store_*.json
    ↓
Step 2: Question Extraction
    → Load contexts → Pre-filter combinations
    → llm_model_func() for question generation
    → Output: questions/{stage}_stage.json, questions/{stage}_stage_ref.json
    ↓
Step 3: Response Generation
    → HyperRAG.aquery() → Query mode routing
    → hyper_query() / naive_query() / etc.
    → Context building → LLM response
    → Output: response/{mode}_{stage}_stage_result.json
```

---

## Summary

The HyperRAG pipeline consists of four sequential steps:

1. **Step 0**: Preprocesses datasets to extract unique contexts for processing
2. **Step 1**: Builds a hypergraph knowledge base by extracting entities and relationships from text, storing them in both graph and vector databases
3. **Step 2**: Generates evaluation questions using LLM based on the contexts
4. **Step 3**: Answers questions using various retrieval strategies, with the "hyper" mode leveraging the full graph structure for the most comprehensive answers

Each step builds upon the previous one, creating a comprehensive RAG system that combines vector similarity search with graph-based knowledge retrieval for improved answer quality.